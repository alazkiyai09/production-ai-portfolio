# Docker Compose configuration for FraudDocs-RAG
#
# This compose file orchestrates:
# - FraudDocs-RAG API service
# - ChromaDB vector database
# - Optional: Ollama for local LLM (development)
#
# Usage:
#   Production: docker-compose up -d
#   Development: docker-compose --profile dev up -d
#   With Ollama: docker-compose --profile ollama up -d
#   Stop: docker-compose down
#   Rebuild: docker-compose up -d --build

version: "3.8"

# ============================================================================
# Networks
# ============================================================================
networks:
  frauddocs-network:
    name: frauddocs-network
    driver: bridge

# ============================================================================
# Volumes
# ============================================================================
volumes:
  chroma-data:
    name: chroma-data
    driver: local
  document-store:
    name: document-store
    driver: local
  model-cache:
    name: model-cache
    driver: local

# ============================================================================
# Services
# ============================================================================
services:
  # ==========================================================================
  # ChromaDB Vector Database
  # ==========================================================================
  chromadb:
    image: chromadb/chroma:0.5.5
    container_name: frauddocs-chromadb
    restart: unless-stopped

    # Environment variables
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8001
      - CHROMA_SERVER_GRPC_PORT=8002
      - CHROMA_LOG_LEVEL=INFO
      # Disable anonymous telemetry
      - ANONYMIZED_TELEMETRY=False

    # Ports
    ports:
      - "${CHROMA_PORT:-8001}:8001"
      - "8002:8002"  # gRPC port

    # Volumes for persistent storage
    volumes:
      - chroma-data:/chroma/chroma
      - model-cache:/root/.cache

    # Networks
    networks:
      - frauddocs-network

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ==========================================================================
  # FraudDocs-RAG API Service
  # ==========================================================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: ${BUILD_TARGET:-runtime}
      args:
        PYTHONUNBUFFERED: 1
        PYTHONDONTWRITEBYTECODE: 1
        APP_USER: appuser
        APP_UID: 1000
        APP_GID: 1000

    image: frauddocs-rag:${IMAGE_TAG:-latest}
    container_name: frauddocs-api
    restart: unless-stopped

    # Environment variables from .env file
    env_file:
      - .env

    # Additional environment variables
    environment:
      # Application settings
      - APP_NAME=FraudDocs-RAG
      - APP_VERSION=1.0.0
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - HOST=0.0.0.0
      - PORT=8000
      - WORKERS=4

      # ChromaDB connection
      - CHROMA_HOST=chromadb
      - CHROMA_PORT=8001
      - CHROMA_PERSIST_DIRECTORY=/app/data/chroma_db
      - CHROMA_COLLECTION_NAME=financial_documents

      # Vector store settings
      - TOP_K_RETRIEVAL=10
      - RERANK_TOP_N=5
      - RERANK_ENABLED=true

      # Embedding model settings
      - EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
      - EMBEDDING_DEVICE=cpu
      - EMBEDDING_BATCH_SIZE=32

      # LLM settings (configure based on your provider)
      # For development with Ollama:
      # - Ollama service must be running
      # - OLLAMA_BASE_URL=http://ollama:11434

      # For demo with GLM-4:
      # - ZHIPUAI_API_KEY=${ZHIPUAI_API_KEY}

      # For production with OpenAI:
      # - OPENAI_API_KEY=${OPENAI_API_KEY}

      # Data paths
      - DATA_DIR=/app/data
      - DOCUMENTS_DIR=/app/data/documents
      - RAW_DOCUMENTS_DIR=/app/data/raw
      - VECTOR_STORE_DIR=/app/data/vector_store
      - LOGS_DIR=/app/logs

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}

      # API settings
      - API_V1_PREFIX=/api/v1
      - ALLOWED_ORIGINS=*

      # CORS settings
      - ENABLE_CORS=true

    # Ports
    ports:
      - "${API_PORT:-8000}:8000"

    # Volumes
    volumes:
      # Document storage
      - document-store:/app/data
      # Model cache
      - model-cache:/home/appuser/.cache
      # Logs (optional - for persistent logs)
      - ./logs:/app/logs

    # Networks
    networks:
      - frauddocs-network

    # Depends on ChromaDB
    depends_on:
      chromadb:
        condition: service_healthy

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 4G
        reservations:
          cpus: "1.0"
          memory: 1G

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

    # Profiles
    profiles:
      - production
      - development

  # ==========================================================================
  # Ollama LLM Service (Development Only)
  # ==========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: frauddocs-ollama
    restart: unless-stopped

    # Ports
    ports:
      - "11434:11434"

    # Volumes for model storage
    volumes:
      - model-cache:/root/.ollama

    # Networks
    networks:
      - fr auddocs-network

    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    # GPU support (uncomment if using NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: "4.0"
          memory: 8G
        reservations:
          cpus: "2.0"
          memory: 4G

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

    # Only start with ollama profile
    profiles:
      - ollama
      - dev

  # ==========================================================================
  # Nginx Reverse Proxy (Optional - Production)
  # ==========================================================================
  nginx:
    image: nginx:alpine
    container_name: frauddocs-nginx
    restart: unless-stopped

    # Ports
    ports:
      - "80:80"
      - "443:443"

    # Volumes
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx

    # Networks
    networks:
      - frauddocs-network

    # Depends on API
    depends_on:
      - api

    # Resource limits
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 128M

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

    # Only start with nginx profile
    profiles:
      - nginx
      - production


# ============================================================================
# Usage Examples
# ============================================================================
#
# 1. Start all services (production):
#    docker-compose up -d
#
# 2. Start with development build:
#    BUILD_TARGET=development docker-compose up -d --build
#
# 3. Start with Ollama for local LLM:
#    docker-compose --profile ollama up -d
#
# 4. Start with Nginx reverse proxy:
#    docker-compose --profile nginx --profile production up -d
#
# 5. View logs:
#    docker-compose logs -f api
#    docker-compose logs -f chromadb
#
# 6. Stop all services:
#    docker-compose down
#
# 7. Stop and remove volumes:
#    docker-compose down -v
#
# 8. Rebuild after code changes:
#    docker-compose up -d --build api
#
# 9. Scale API service (multiple workers):
#    docker-compose up -d --scale api=3
#
# 10. Run commands in container:
#     docker-compose exec api bash
#     docker-compose exec api python -m pytest
#
