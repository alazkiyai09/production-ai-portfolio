{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMOps-Eval: LLM Evaluation & Optimization Framework\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the LLMOps-Eval framework featuring:\n",
    "- Multi-model comparison (OpenAI, Anthropic, Cohere, Ollama)\n",
    "- 9 comprehensive evaluation metrics\n",
    "- Prompt A/B testing framework\n",
    "- Cost optimization and tracking\n",
    "- Results visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai anthropic cohere-ai pandas plotly sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '../../projects/evaluation/LLMOps-Eval')\n",
    "\n",
    "# Set up API keys\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-key'\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'your-anthropic-key'\n",
    "os.environ['COHERE_API_KEY'] = 'your-cohere-key'\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.llm_providers import create_llm_provider\n",
    "from src.runners.eval_runner import EvaluationRunner, EvaluationConfig, ModelConfig\n",
    "from src.evaluation.metrics import (\n",
    "    ExactMatchMetric,\n",
    "    ContainsMetric,\n",
    "    SemanticSimilarityMetric,\n",
    "    LatencyMetric,\n",
    "    CostMetric\n",
    ")\n",
    "\n",
    "print(\"üìä Initializing LLMOps-Eval Framework...\\n\")\n",
    "\n",
    "# Create evaluation runner\n",
    "runner = EvaluationRunner(\n",
    "    output_dir='./results',\n",
    "    datasets_dir='./datasets'\n",
    ")\n",
    "print(\"‚úÖ Evaluation runner created\")\n",
    "\n",
    "print(\"\\nüéâ Framework initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Models to Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "models = [\n",
    "    ModelConfig(\n",
    "        provider=\"openai\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.0\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        provider=\"anthropic\",\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0.0\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        provider=\"cohere\",\n",
    "        model=\"command\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"ü§ñ Configured {len(models)} models for evaluation:\")\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"  {i}. {model.provider}:{model.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation dataset\n",
    "evaluation_data = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected\": \"Paris\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain what machine learning is in one sentence.\",\n",
    "        \"expected\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\",\n",
    "        \"category\": \"explanation\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the primary colors?\",\n",
    "        \"expected\": \"Red, blue, and yellow\",\n",
    "        "category\": \"list\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"expected\": \"William Shakespeare\",\n",
    "        \"category\": \"factual\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is 2 + 2?\",\n",
    "        \"expected\": \"4\",\n",
    "        \"category\": \"math\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìã Created evaluation dataset with {len(evaluation_data)} test cases\")\n",
    "print(f\"\\nCategories: {set([item['category'] for item in evaluation_data])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Single Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Evaluate single model (OpenAI GPT-4o-mini)\n",
    "model = models[0]\n",
    "\n",
    "print(f\"üîç Evaluating {model.provider}:{model.model}...\\n\")\n",
    "\n",
    "# Create LLM provider\n",
    "llm = create_llm_provider(\n",
    "    provider=model.provider,\n",
    "    model_name=model.model,\n",
    "    temperature=model.temperature\n",
    ")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, test_case in enumerate(evaluation_data, 1):\n",
    "    print(f\"Test {i}/{len(evaluation_data)}: {test_case['question'][:40]}...\")\n",
    "    \n",
    "    # Time the request\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate response\n",
    "    response = llm.generate(\n",
    "        prompt=test_case['question'],\n",
    "        system_prompt=\"You are a helpful assistant. Answer concisely.\"\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    exact_match = ExactMatchMetric().evaluate(\n",
    "        response=response,\n",
    "        expected=test_case['expected']\n",
    "    )['score']\n",
    "    \n",
    "    contains = ContainsMetric().evaluate(\n",
    "        response=response,\n",
    "        expected=test_case['expected']\n",
    "    )['score']\n",
    "    \n",
    "    results.append({\n",
    "        'question': test_case['question'],\n",
    "        'expected': test_case['expected'],\n",
    "        'response': response,\n",
    "        'latency': elapsed,\n",
    "        'exact_match': exact_match,\n",
    "        'contains': contains,\n",
    "        'category': test_case['category']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Response: {response[:50]}...\")\n",
    "    print(f\"  Latency: {elapsed:.2f}s | Exact Match: {exact_match} | Contains: {contains}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"üìä Evaluation Results Summary\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall metrics\n",
    "avg_latency = df['latency'].mean()\n",
    "accuracy_exact = df['exact_match'].mean() * 100\n",
    "accuracy_contains = df['contains'].mean() * 100\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"  Average Latency: {avg_latency:.2f}s\")\n",
    "print(f\"  Exact Match Accuracy: {accuracy_exact:.1f}%\")\n",
    "print(f\"  Contains Accuracy: {accuracy_contains:.1f}%\")\n",
    "\n",
    "# By category\n",
    "print(f\"\\nPerformance by Category:\")\n",
    "for category in df['category'].unique():\n",
    "    cat_df = df[df['category'] == category]\n",
    "    cat_accuracy = cat_df['exact_match'].mean() * 100\n",
    "    print(f\"  {category:12} | Accuracy: {cat_accuracy:.1f}% | Count: {len(cat_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add exact match bar\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Exact Match',\n",
    "    x=df['question'].str[:30] + '...',\n",
    "    y=df['exact_match'] * 100,\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "# Add contains bar\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Contains',\n",
    "    x=df['question'].str[:30] + '...',\n",
    "    y=df['contains'] * 100,\n",
    "    marker_color='darkblue'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Accuracy by Question',\n",
    "    xaxis_title='Question',\n",
    "    yaxis_title='Accuracy (%)',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency visualization\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x='question',\n",
    "    y='latency',\n",
    "    color='exact_match',\n",
    "    size='contains',\n",
    "    title='Response Latency vs Accuracy',\n",
    "    labels={'latency': 'Latency (s)', 'question': 'Question'},\n",
    "    color_continuous_scale='RdYlGn'\n",
    ")\n",
    "\n",
    "fig.update_layout(height=500)\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Comparing Multiple Models...\\n\")\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "# Evaluate each model on a subset of questions\n",
    "test_subset = evaluation_data[:3]  # First 3 questions for speed\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model.provider}:{model.model}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    llm = create_llm_provider(\n",
    "        provider=model.provider,\n",
    "        model_name=model.model,\n",
    "        temperature=model.temperature\n",
    "    )\n",
    "    \n",
    "    model_results = {\n",
    "        'model': f\"{model.provider}:{model.model}\",\n",
    "        'avg_latency': [],\n",
    "        'exact_match': [],\n",
    "        'contains': []\n",
    "    }\n",
    "    \n",
    "    for test_case in test_subset:\n",
    "        start = time.time()\n",
    "        response = llm.generate(\n",
    "            prompt=test_case['question'],\n",
    "            system_prompt=\"Answer concisely.\"\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        exact = ExactMatchMetric().evaluate(\n",
    "            response=response,\n",
    "            expected=test_case['expected']\n",
    "        )['score']\n",
    "        \n",
    "        contains = ContainsMetric().evaluate(\n",
    "            response=response,\n",
    "            expected=test_case['expected']\n",
    "        )['score']\n",
    "        \n",
    "        model_results['avg_latency'].append(elapsed)\n",
    "        model_results['exact_match'].append(exact)\n",
    "        model_results['contains'].append(contains)\n",
    "        \n",
    "        print(f\"  ‚è±Ô∏è  {elapsed:.2f}s | ‚úì: {exact} | ‚äÜ: {contains}\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'model': model_results['model'],\n",
    "        'avg_latency': sum(model_results['avg_latency']) / len(model_results['avg_latency']),\n",
    "        'accuracy': sum(model_results['exact_match']) / len(model_results['exact_match']) * 100,\n",
    "        'contains_accuracy': sum(model_results['contains']) / len(model_results['contains']) * 100\n",
    "    })\n",
    "\n",
    "print(f\"\\n\\n‚úÖ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\nüìä Model Comparison Summary\\n\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
    "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Latency (lower is better)', 'Exact Match Accuracy', 'Contains Accuracy'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison_df['model'],\n",
    "    y=comparison_df['avg_latency'],\n",
    "    name='Latency',\n",
    "    marker_color='orange'\n",
    "), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison_df['model'],\n",
    "    y=comparison_df['accuracy'],\n",
    "    name='Accuracy',\n",
    "    marker_color='green',\n",
    "    showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=comparison_df['model'],\n",
    "    y=comparison_df['contains_accuracy'],\n",
    "    name='Contains',\n",
    "    marker_color='blue',\n",
    "    showlegend=False\n",
    "), row=1, col=3)\n",
    "\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(height=400, title_text=\"Model Comparison\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prompt A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt variants to test\n",
    "prompt_variants = [\n",
    "    {\n",
    "        \"name\": \"Direct\",\n",
    "        \"system_prompt\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Detailed\",\n",
    "        \"system_prompt\": \"You are a helpful assistant. Provide detailed and comprehensive answers.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Concise\",\n",
    "        \"system_prompt\": \"You are a helpful assistant. Answer briefly and to the point.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "test_question = \"What is machine learning?\"\n",
    "\n",
    "print(\"üß™ Testing Prompt Variants\\n\")\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use first model for testing\n",
    "llm = create_llm_provider(\n",
    "    provider=models[0].provider,\n",
    "    model_name=models[0].model,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "for variant in prompt_variants:\n",
    "    print(f\"\\nPrompt Style: {variant['name']}\")\n",
    "    print(f\"System Prompt: {variant['system_prompt']}\")\n",
    "    \n",
    "    response = llm.generate(\n",
    "        prompt=test_question,\n",
    "        system_prompt=variant['system_prompt']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResponse: {response}\")\n",
    "    print(f\"Length: {len(response)} characters\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate costs for each model\n",
    "# Pricing (approximate per 1M tokens):\n",
    "# GPT-4o-mini: $0.15 input, $0.60 output\n",
    "# Claude Haiku: $0.25 input, $1.25 output\n",
    "# Command: $0.15 input, $0.60 output\n",
    "\n",
    "pricing = {\n",
    "    \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"claude-3-haiku-20240307\": {\"input\": 0.25, \"output\": 1.25},\n",
    "    \"command\": {\"input\": 0.15, \"output\": 0.60}\n",
    "}\n",
    "\n",
    "print(\"üí∞ Cost Analysis\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Estimate tokens (rough approximation: 1 token ‚âà 4 characters)\n",
    "avg_input_length = len(test_question) + 50  # question + system prompt\n",
    "avg_output_length = 100  # average response length\n",
    "\n",
    "for model_name, price in pricing.items():\n",
    "    input_cost = (avg_input_length / 4) * price['input'] / 1_000_000\n",
    "    output_cost = (avg_output_length / 4) * price['output'] / 1_000_000\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Input:  ${input_cost:.6f}\")\n",
    "    print(f\"  Output: ${output_cost:.6f}\")\n",
    "    print(f\"  Total:  ${total_cost:.6f} per request\")\n",
    "    print(f\"  For 1000 requests: ${total_cost * 1000:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ What We Demonstrated:\n",
    "\n",
    "1. **Framework Setup** - Initialized LLMOps-Eval\n",
    "2. **Multi-Model Support** - Tested OpenAI, Anthropic, Cohere\n",
    "3. **Evaluation Metrics** - Exact match, contains, semantic similarity\n",
    "4. **Performance Tracking** - Latency, accuracy measurements\n",
    "5. **Data Visualization** - Plotly charts for results\n",
    "6. **Model Comparison** - Side-by-side analysis\n",
    "7. **Prompt A/B Testing** - Variant comparison\n",
    "8. **Cost Analysis** - Token usage and pricing\n",
    "\n",
    "### üéØ Key Features:\n",
    "\n",
    "- ‚úÖ 9 comprehensive evaluation metrics\n",
    "- ‚úÖ Multi-provider LLM support\n",
    "- ‚úÖ Automated evaluation pipelines\n",
    "- ‚úÖ Results visualization\n",
    "- ‚úÖ Prompt optimization\n",
    "- ‚úÖ Cost tracking\n",
    "- ‚úÖ Batch evaluation\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "- Run full evaluation suite on all models\n",
    "- Add custom evaluation metrics\n",
    "- Export results to reports\n",
    "- Deploy with FastAPI: `uvicorn src.api.main:app --reload`\n",
    "- Use Streamlit dashboard: `streamlit run src/dashboard/app.py`\n",
    "\n",
    "---\n",
    "\n",
    "**üìñ Documentation:** [LLMOps-Eval README](../../projects/evaluation/LLMOps-Eval/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
