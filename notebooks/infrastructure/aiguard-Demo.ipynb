{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aiguard: AI Safety & Content Moderation\n",
    "\n",
    "## Overview\n",
    "AI safety system for content moderation and compliance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fastapi transformers torch\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../projects/infrastructure/aiguard')\n",
    "\n",
    "print(\"âœ… aiguard setup complete\")\n",
    "print(\"\\nðŸ›¡ï¸ AI Safety Features:\")\n",
    "  â€¢ Content moderation\n",
    "  â€¢ Safety filter detection\n",
    "   â€¢ Toxic content flagging\n",
    "  â€¢ PII detection\n",
    "  â€¢ Bias monitoring\n",
    "  â€¢ Compliance checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Content Safety Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_content_safety(text: str) -> dict:\n",
    "    \"\"\"Check content for safety issues.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for toxic patterns\n",
    "    toxic_patterns = ['hate', 'violence', 'threat', 'harassment']\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in toxic_patterns:\n",
    "        if pattern in text_lower:\n",
    "            issues.append(f\"Toxic content: {pattern}\")\n",
    "    \n",
    "    # Check for PII patterns\n",
    "    import re\n",
    "    if re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text):  # SSN\n",
    "        issues.append(\"Possible SSN detected\")\n",
    "    if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text):  # Email\n",
    "        issues.append(\"Email detected\")\n",
    "    \n",
    "    is_safe = len(issues) == 0\n",
    "    safety_score = max(0, 100 - len(issues) * 25)\n",
    "    \n",
    "    return {\n",
    "        'is_safe': is_safe,\n",
    "        'safety_score': safety_score,\n",
    "        'issues': issues\n",
    "    }\n",
    "\n",
    "# Test content safety\n",
    "test_contents = [\n",
    "    \"This is a safe, helpful message for users.\",\n",
    "    \"This content includes hate speech and threats.\",\n",
    "    \"Contact us at support@example.com or call 555-123-4567.\",\n",
    "    \"Here's the information you requested.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ›¡ï¸ Content Safety Checks\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for content in test_contents:\n",
    "    result = check_content_safety(content)\n",
    "    status = \"âœ… SAFE\" if result['is_safe'] else \"âš ï¸  BLOCKED\"\n",
    "    \n",
    "    print(f\"\\nContent: {content[:50]}...\")\n",
    "    print(f\"{status} (Score: {result['safety_score']}/100)\")\n",
    "    \n",
    "    if result['issues']:\n",
    "        print(f\"Issues: {', '.join(result['issues'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bias(text: str) -> dict:\n",
    "    \"\"\"Detect potential bias in text.\"\"\"\n",
    "    bias_indicators = []\n",
    "    \n",
    "    # Gender bias patterns\n",
    "    gender_patterns = [\n",
    "        (r'\\b(men|women|guys|girls)\\s+(are|can\\s+not)', 'Gender stereotyping'),\n",
    "        (r'\\b(naturally|inherently)\\s+(suited|better|worse)', 'Inherent ability bias'),\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern, bias_type in gender_patterns:\n",
    "        if re.search(pattern, text_lower):\n",
    "            bias_indicators.append(bias_type)\n",
    "    \n",
    "    return {\n",
    "        'has_bias': len(bias_indicators) > 0,\n",
    "        'bias_indicators': bias_indicators,\n",
    "        'bias_score': len(bias_indicators) * 20\n",
    "    }\n",
    "\n",
    "# Test bias detection\n",
    "test_texts = [\n",
    "    \"Both men and women are qualified for this role.\",\n",
    "    \"Guys are naturally better at technical tasks.\",\n",
    "    \"The system processes data efficiently and accurately.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š Bias Detection\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in test_texts:\n",
    "    result = detect_bias(text)\n",
    "    status = \"âš ï¸  Potential Bias\" if result['has_bias'] else \"âœ… No Bias Detected\"\n",
    "    \n",
    "    print(f\"\\nText: {text}\\n\")\n",
    "    print(f\"{status}\")\n",
    "    \n",
    "    if result['bias_indicators']:\n",
    "        print(f\"Indicators: {', '.join(result['bias_indicators'])}\")\n",
    "    print(f\"Bias Score: {result['bias_score']}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compliance Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_compliance(text: str, regulations: list) -> dict:\n",
    "    \"\"\"Check text against compliance regulations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for regulation in regulations:\n",
    "        if regulation == 'GDPR':\n",
    "            # Check for GDPR compliance\n",
    "            has_pii = bool(re.search(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', text))\n",
    "            has_email = bool(re.search(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}', text))\n",
    "            results['GDPR'] = {\n",
    "                'compliant': not (has_pii or has_email),\n",
    "                'issues': [],\n",
    "            }\n",
    "            if has_pii:\n",
    "                results['GDPR']['issues'].append('Personal data detected')\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test compliance\n",
    "compliance_text = \"User John Doe (john@example.com, SSN: 123-45-6789) submitted a complaint.\"\n",
    "\n",
    "print(\"ðŸ“‹ Compliance Checking\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = check_compliance(compliance_text, ['GDPR'])\n",
    "print(f\"Regulation: GDPR\")\n",
    "print(f\"Compliant: {result['GDPR']['compliant']}\")\n",
    "print(f\"Issues: {result['GDPR']['issues']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ðŸŽ¯ Key Features:\n",
    "- âœ… Content moderation\n",
    "- âœ… Toxic content flagging\n",
    "   - PII detection\n",
    "- âœ… Bias monitoring\n",
    "- âœ… Compliance checking\n",
    "- âœ… Safety scoring\n",
    "\n",
    "### ðŸš€ Quick Start:\n",
    "```bash\n",
    "cd ../../projects/infrastructure/aiguard\n",
    "python -m src.main\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
