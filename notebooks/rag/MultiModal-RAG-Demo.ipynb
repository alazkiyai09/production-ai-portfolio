{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiModal-RAG: Image + Text Retrieval System\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the MultiModal-RAG system featuring:\n",
    "- Image and text embedding (CLIP/ViT)\n",
    "- Multi-modal vector search\n",
    "- Image + document retrieval\n",
    "- Cross-modal query processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q llama-index chromadb sentence-transformers pillow torch torchvision transformers\n",
    "!pip install -q clip-by-openai  # For CLIP model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '../../projects/rag/MultiModal-RAG')\n",
    "\n",
    "# Set up environment\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize MultiModal RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üñºÔ∏è Initializing MultiModal-RAG System...\\n\")\n",
    "\n",
    "# For demonstration, we'll simulate the multimodal system\n",
    "# In production, this would use the actual project modules\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(\"‚úÖ MultiModal RAG components initialized\")\n",
    "print(\"\\nüéØ Key Features:\")\n",
    "print(\"  ‚Ä¢ Image embeddings (CLIP/ViT)\")\n",
    "print(\"  ‚Ä¢ Text embeddings (sentence-transformers)\")\n",
    "print(\"  ‚Ä¢ Cross-modal retrieval\")\n",
    "print(\"  ‚Ä¢ Image + document search\")\n",
    "print(\"  ‚Ä¢ Multi-modal query processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample MultiModal Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with images and text\n",
    "multimodal_docs = [\n",
    "    {\n",
    "        \"type\": \"image\",\n",
    "        \"description\": \"Data pipeline architecture diagram\",\n",
    "        \"content\": \"Architecture showing ETL process with data flow from sources through transformations to warehouse\",\n",
    "        \"tags\": [\"architecture\", \"pipeline\", \"ETL\", \"diagram\"]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"description\": \"Data processing documentation\",\n",
    "        \"content\": \"ETL pipelines extract data from sources, transform it according to business rules, and load it into the data warehouse. This process runs daily.\",\n",
    "        \"tags\": [\"documentation\", \"ETL\", \"data\"]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"image\",\n",
    "        \"description\": \"Dashboard screenshot\",\n",
    "        \"content\": \"Analytics dashboard showing real-time metrics with charts and KPIs for business performance tracking\",\n",
    "        \"tags\": [\"dashboard\", \"analytics\", \"metrics\", \"visualization\"]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"description\": \"Dashboard user guide\",\n",
    "        \"content\": \"The analytics dashboard provides real-time visibility into key performance indicators. Users can filter by date range, department, and metric type.\",\n",
    "        \"tags\": [\"guide\", \"dashboard\", \"user manual\"]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"image\",\n",
    "        \"description\": \"Network topology diagram\",\n",
    "        \"content\": \"Network architecture showing load balancers, web servers, application servers, and database clusters with security layers\",\n",
    "        \"tags\": [\"network\", \"infrastructure\", \"security\", \"architecture\"]\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"description\": \"Security configuration guide\",\n",
    "        \"content\": \"Network security implements defense in depth with firewalls, intrusion detection, and access controls. All traffic is encrypted in transit.\",\n",
    "        \"tags\": [\"security\", \"network\", \"configuration\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìÑ Created {len(multimodal_docs)} multimodal documents\")\n",
    "print(f\"  ‚Ä¢ Image docs: {sum(1 for d in multimodal_docs if d['type'] == 'image')}\")\n",
    "print(f\"  ‚Ä¢ Text docs: {sum(1 for d in multimodal_docs if d['type'] == 'text')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulated MultiModal Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def simulate_text_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Simulate text embedding (in production, uses sentence-transformers).\"\"\"\n",
    "    # In production: return embedding_model.encode(text)\n",
    "    # For demo, create a deterministic hash-based vector\n",
    "    hash_val = hash(text)\n",
    "    np.random.seed(hash_val % 10000)\n",
    "    return np.random.rand(384)  # Standard embedding size\n",
    "\n",
    "def simulate_image_embedding(image_description: str) -> np.ndarray:\n",
    "    \"\"\"Simulate image embedding (in production, uses CLIP).\"\"\"\n",
    "    # In production: return clip_model.encode(image)\n",
    "    # For demo, create a deterministic hash-based vector\n",
    "    hash_val = hash(image_description)\n",
    "    np.random.seed(hash_val % 10000 + 5000)\n",
    "    return np.random.rand(512)  # CLIP embedding size\n",
    "\n",
    "# Create embeddings for all documents\n",
    "print(\"üî¢ Generating embeddings...\\n\")\n",
    "\n",
    "for doc in multimodal_docs:\n",
    "    if doc['type'] == 'image':\n",
    "        doc['embedding'] = simulate_image_embedding(doc['content'][:100])\n",
    "        doc['embedding_dim'] = 512\n",
    "        print(f\"  üì∏ {doc['description'][:30]:30} | Image embedding (512d)\")\n",
    "    else:\n",
    "        doc['embedding'] = simulate_text_embedding(doc['content'][:100])\n",
    "        doc['embedding_dim'] = 384\n",
    "        print(f\"  üìÑ {doc['description'][:30]:30} | Text embedding (384d)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated embeddings for {len(multimodal_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MultiModal Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def multimodal_query(query: str, query_type: str = \"text\", docs: List[Dict] = None, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Perform multi-modal search across documents.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        query_type: 'text' or 'image' search\n",
    "        docs: Document list\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of ranked documents\n",
    "    \"\"\"\n",
    "    if docs is None:\n",
    "        docs = multimodal_docs\n",
    "    \n",
    "    # Generate query embedding\n",
    "    if query_type == \"text\":\n",
    "        query_emb = simulate_text_embedding(query)\n",
    "    else:\n",
    "        query_emb = simulate_image_embedding(query)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    results = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Pad smaller embeddings to match query size if needed\n",
    "        doc_emb = doc['embedding']\n",
    "        \n",
    "        # For cross-modal search, project to common dimension\n",
    "        if query_emb.shape[0] != doc_emb.shape[0]:\n",
    "            # Simple projection for demo (in production, use learned projection)\n",
    "            max_dim = max(query_emb.shape[0], doc_emb.shape[0])\n",
    "            if query_emb.shape[0] < max_dim:\n",
    "                query_emb_padded = np.pad(query_emb, (0, max_dim - query_emb.shape[0]))\n",
    "            else:\n",
    "                query_emb_padded = query_emb[:max_dim]\n",
    "            if doc_emb.shape[0] < max_dim:\n",
    "                doc_emb_padded = np.pad(doc_emb, (0, max_dim - doc_emb.shape[0]))\n",
    "            else:\n",
    "                doc_emb_padded = doc_emb[:max_dim]\n",
    "        else:\n",
    "            query_emb_padded = query_emb\n",
    "            doc_emb_padded = doc_emb\n",
    "        \n",
    "        similarity = cosine_similarity([query_emb_padded], [doc_emb_padded])[0][0]\n",
    "        \n",
    "        results.append({\n",
    "            'document': doc,\n",
    "            'score': float(similarity),\n",
    "            'type': doc['type']\n",
    "        })\n",
    "    \n",
    "    # Sort by similarity\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return results[:top_k]\n",
    "\n",
    "print(\"üîç MultiModal query system ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Queries (Find Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text queries that find relevant images\n",
    "text_queries = [\n",
    "    \"dashboard analytics charts\",\n",
    "    \"network security infrastructure\",\n",
    "    \"data pipeline architecture\"\n",
    "]\n",
    "\n",
    "print(\"üìù Text Queries Finding Images\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for query in text_queries:\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    \n",
    "    results = multimodal_query(query, query_type=\"text\", top_k=3)\n",
    "    \n",
    "    print(f\"\\nTop {len(results)} results:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        doc = result['document']\n",
    "        icon = \"üì∏\" if result['type'] == 'image' else \"üìÑ\"\n",
    "        print(f\"  {i}. {icon} {doc['description']}\")\n",
    "        print(f\"     Type: {result['type']}\")\n",
    "        print(f\"     Score: {result['score']:.3f}\")\n",
    "        print(f\"     Tags: {', '.join(doc['tags'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Modal Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Cross-Modal Search Demo\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Scenario 1: Text query finds images\n",
    "print(\"\\nüìù ‚Üí üì∏ : Text query finds images\")\n",
    "query1 = \"architecture diagram\"\n",
    "results1 = multimodal_query(query1, query_type=\"text\", top_k=2)\n",
    "\n",
    "print(f\"Query: '{query1}'\")\n",
    "for r in results1:\n",
    "    print(f\"  ‚Ä¢ {r['document']['description']} ({r['type']}, score: {r['score']:.3f})\")\n",
    "\n",
    "# Scenario 2: Image query finds text\n",
    "print(\"\\nüì∏ ‚Üí üìÑ : Image description finds text documents\")\n",
    "query2 = \"dashboard metrics visualization\"\n",
    "results2 = multimodal_query(query2, query_type=\"text\", top_k=2)\n",
    "\n",
    "print(f\"Query: '{query2}'\")\n",
    "for r in results2:\n",
    "    print(f\"  ‚Ä¢ {r['document']['description']} ({r['type']}, score: {r['score']:.3f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Cross-modal search allows any query type to find any document type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MultiModal RAG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_rag_generation(query: str, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Generate response using retrieved multimodal documents.\n",
    "    \n",
    "    In production, this would use an LLM that can see images.\n",
    "    For demo, we simulate the response.\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    for doc in retrieved_docs:\n",
    "        if doc['type'] == 'image':\n",
    "            context_parts.append(f\"[Image: {doc['description']}] {doc['content']}\")\n",
    "        else:\n",
    "            context_parts.append(f\"[Document: {doc['description']}] {doc['content']}\")\n",
    "    \n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # Simulated RAG generation\n",
    "    response = f\"\"\"Based on the retrieved information, here's what I found:\\n",
    "\\n",
    "{context}\\n",
    "\\n",
    "This combines information from {len(retrieved_docs)} multimodal sources.\n",
    "Note: In production, the LLM would be able to actually see and analyze the images.\n",
    "\"\"\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"ü§ñ MultiModal RAG Generation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multimodal RAG\n",
    "query = \"What does the analytics dashboard show?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query}\\n\")\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved = multimodal_query(query, top_k=2)\n",
    "\n",
    "print(f\"üîç Retrieved {len(retrieved)} documents:\\n\")\n",
    "for r in retrieved:\n",
    "    doc = r['document']\n",
    "    icon = \"üì∏\" if r['type'] == 'image' else \"üìÑ\"\n",
    "    print(f\"  {icon} {doc['description']} (score: {r['score']:.3f})\")\n",
    "\n",
    "# Generate response\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nü§ñ RAG Response:\\n\")\n",
    "\n",
    "response = multimodal_rag_generation(query, [r['document'] for r in retrieved])\n",
    "print(response[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: Text vs MultiModal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Feature': ['Text Embeddings', 'Image Embeddings', 'Cross-Modal Search', \n",
    "                'Document Types', 'Use Cases'],\n",
    "    'Text-Only RAG': ['sentence-transformers', 'N/A', 'N/A', 'Text only', 'Text documents'],\n",
    "    'MultiModal RAG': ['sentence-transformers', 'CLIP/ViT', 'Yes', 'Text + Images', \n",
    "                    'Product docs, medical imaging, visual search']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nüìä Text-Only vs MultiModal RAG\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Performance Considerations\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "metrics = {\n",
    "    'Image Embedding': {\n",
    "        'Model': 'CLIP ViT-B/32',\n",
    "        'Dimension': '512d',\n",
    "        'Time': '~200ms per image',\n",
    "        'Size': '~600MB'\n",
    "    },\n",
    "    'Text Embedding': {\n",
    "        'Model': 'sentence-transformers',\n",
    "        'Dimension': '384d',\n",
    "        'Time': '~50ms per doc',\n",
    "        'Size': '~420MB'\n",
    "    },\n",
    "    'Cross-Modal Search': {\n",
    "        'Method': 'Projection space',\n",
    "        'Overhead': '+10-20% vs single-modal',\n",
    "        'Benefit': 'Unified search across all content types'\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, value in metrics.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    for k, v in value.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ What We Demonstrated:\n",
    "\n",
    "1. **MultiModal Embeddings** - Image and text embeddings\n",
    "2. **Cross-Modal Search** - Text queries find images, image queries find text\n",
    "3. **MultiModal RAG** - Generation using retrieved images and text\n",
    "4. **Performance** - Considerations for multi-modal systems\n",
    "\n",
    "### üéØ Key Features:\n",
    "\n",
    "- ‚úÖ CLIP/ViT image embeddings\n",
    "- ‚úÖ Sentence transformer text embeddings\n",
    "- ‚úÖ Cross-modal retrieval\n",
    "- ‚úÖ Image + text documents\n",
    "- ‚úÖ Unified vector search\n",
    "- ‚úÖ Multi-modal generation\n",
    "\n",
    "### üìö Use Cases:\n",
    "\n",
    "- **Product Documentation**: Search manuals with diagrams\n",
    "- **Medical Imaging**: Find relevant X-rays with reports\n",
    "- **E-commerce**: Visual search with product descriptions\n",
    "- **Education**: Textbooks with diagrams and illustrations\n",
    "- **Real Estate**: Property listings with photos and descriptions\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "- Deploy: `cd ../../projects/rag/MultiModal-RAG && python -m src.main`\n",
    "- Try with real images and CLIP embeddings\n",
    "- Implement learned projection for cross-modal search\n",
    "- Add vision capabilities to LLM generation\n",
    "\n",
    "---\n",
    "\n",
    "**üìñ Documentation:** [MultiModal-RAG README](../../projects/rag/MultiModal-RAG/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
