{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise-RAG: Production-Grade Hybrid RAG System\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates the Enterprise-RAG system with:\n",
    "- Hybrid retrieval (dense vector + sparse BM25)\n",
    "- Cross-encoder reranking\n",
    "- Multi-format document ingestion\n",
    "- RAGAS evaluation metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q llama-index chromadb sentence-transformers rank-bm25 python-docx pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '../../projects/rag/Enterprise-RAG')\n",
    "\n",
    "# Set up environment\n",
    "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Replace with actual key\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ingestion import create_processor_from_settings\n",
    "from src.retrieval import create_vector_store, create_embedding_service, create_hybrid_retriever\n",
    "from src.retrieval.sparse_retriever import create_bm25_retriever\n",
    "from src.retrieval.reranker import CrossEncoderReranker\n",
    "from src.generation import create_rag_chain\n",
    "\n",
    "print(\"üìö Initializing components...\")\n",
    "\n",
    "# 1. Create embedding service\n",
    "embedding_service = create_embedding_service()\n",
    "print(\"‚úÖ Embedding service initialized\")\n",
    "\n",
    "# 2. Create vector store\n",
    "vector_store = create_vector_store()\n",
    "print(\"‚úÖ Vector store created\")\n",
    "\n",
    "# 3. Create BM25 sparse retriever\n",
    "from src.retrieval.sparse_retriever import create_bm25_retriever\n",
    "bm25_retriever = create_bm25_retriever()\n",
    "print(\"‚úÖ BM25 retriever created\")\n",
    "\n",
    "# 4. Create hybrid retriever\n",
    "hybrid_retriever = create_hybrid_retriever(\n",
    "    vector_store=vector_store,\n",
    "    embedding_service=embedding_service,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "print(\"‚úÖ Hybrid retriever created\")\n",
    "\n",
    "# 5. Create reranker\n",
    "reranker = CrossEncoderReranker()\n",
    "print(\"‚úÖ Cross-encoder reranker initialized\")\n",
    "\n",
    "# 6. Create document processor\n",
    "document_processor = create_processor_from_settings()\n",
    "print(\"‚úÖ Document processor initialized\")\n",
    "\n",
    "# 7. Create RAG chain\n",
    "rag_chain = create_rag_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    reranker=reranker\n",
    ")\n",
    "print(\"‚úÖ RAG chain created\")\n",
    "\n",
    "print(\"\\nüéâ All components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents\n",
    "sample_docs = [\n",
    "    {\n",
    "        \"text\": \"\"\"Enterprise RAG systems combine dense and sparse retrieval for optimal results.\n",
    "        Dense retrieval uses vector embeddings for semantic understanding, while sparse retrieval\n",
    "        like BM25 provides exact keyword matching. The hybrid approach combines both methods.\"\"\",\n",
    "        \"metadata\": {\"source\": \"rag_guide.pdf\", \"page\": 1}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"Cross-encoder reranking significantly improves retrieval accuracy by re-scoring\n",
    "        the top-k results from the initial retrieval phase. This two-stage approach balances\n",
    "        efficiency with accuracy.\"\"\",\n",
    "        \"metadata\": {\"source\": \"reranking_doc.pdf\", \"page\": 3}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"RAGAS (Retrieval Augmented Generation Assessment) provides comprehensive metrics\n",
    "        including faithfulness, answer relevancy, context precision, and context recall to evaluate\n",
    "        RAG systems.\"\"\",\n",
    "        \"metadata\": {\"source\": \"evaluation_metrics.pdf\", \"page\": 5}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìÑ Preparing to ingest {len(sample_docs)} documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and add documents\n",
    "from llama_index import Document\n",
    "\n",
    "for i, doc_data in enumerate(sample_docs, 1):\n",
    "    # Create Document object\n",
    "    doc = Document(\n",
    "        text=doc_data[\"text\"],\n",
    "        metadata=doc_data[\"metadata\"]\n",
    "    )\n",
    "    \n",
    "    # Add to retriever\n",
    "    hybrid_retriever.add_documents([doc])\n",
    "    \n",
    "    print(f\"‚úÖ Ingested document {i}: {doc_data['metadata']['source']}\")\n",
    "\n",
    "print(f\"\\nüéâ Successfully ingested {len(sample_docs)} documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Hybrid retrieval explanation\n",
    "query1 = \"What is hybrid retrieval in RAG systems?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query1}\\n\")\n",
    "print(\"üîç Searching...\\n\")\n",
    "\n",
    "response1 = rag_chain.query(\n",
    "    query1,\n",
    "    top_k=3,\n",
    "    use_reranking=True\n",
    ")\n",
    "\n",
    "print(f\"üìù Answer:\\n{response1.answer}\\n\")\n",
    "print(f\"üìö Sources:\")\n",
    "for i, source in enumerate(response1.sources, 1):\n",
    "    print(f\"  {i}. {source['metadata']['source']} (relevance: {source.get('score', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: Reranking benefits\n",
    "query2 = \"How does cross-encoder reranking improve RAG systems?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query2}\\n\")\n",
    "print(\"üîç Searching...\\n\")\n",
    "\n",
    "response2 = rag_chain.query(\n",
    "    query2,\n",
    "    top_k=2,\n",
    "    use_reranking=True\n",
    ")\n",
    "\n",
    "print(f\"üìù Answer:\\n{response2.answer}\\n\")\n",
    "print(f\"‚è±Ô∏è Processing time: {response2.metadata.get('processing_time', 'N/A')}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Evaluation metrics\n",
    "query3 = \"What metrics does RAGAS provide for evaluation?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query3}\\n\")\n",
    "print(\"üîç Searching...\\n\")\n",
    "\n",
    "response3 = rag_chain.query(\n",
    "    query3,\n",
    "    top_k=3,\n",
    "    use_reranking=False  # Test without reranking\n",
    ")\n",
    "\n",
    "print(f\"üìù Answer:\\n{response3.answer}\\n\")\n",
    "print(f\"üîÑ Reranking used: {response3.metadata.get('use_reranking', False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"How does RAGAS evaluate RAG systems?\"\n",
    "\n",
    "print(f\"üîç Test Query: {test_query}\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dense only\n",
    "print(\"\\n1Ô∏è‚É£ Dense Retrieval Only:\")\n",
    "dense_results = vector_store.query(test_query, top_k=3)\n",
    "for i, result in enumerate(dense_results, 1):\n",
    "    print(f\"  {i}. {result.metadata['source']} - score: {result.score:.3f}\")\n",
    "\n",
    "# Sparse only\n",
    "print(\"\\n2Ô∏è‚É£ Sparse (BM25) Retrieval Only:\")\n",
    "sparse_results = bm25_retriever.retrieve(test_query, top_k=3)\n",
    "for i, result in enumerate(sparse_results, 1):\n",
    "    print(f\"  {i}. {result.metadata['source']} - score: {result.score:.3f}\")\n",
    "\n",
    "# Hybrid\n",
    "print(\"\\n3Ô∏è‚É£ Hybrid Retrieval (Dense + Sparse):\")\n",
    "hybrid_results = hybrid_retriever.retrieve(test_query, top_k=3)\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"  {i}. {result.metadata['source']} - score: {result.score:.3f}\")\n",
    "\n",
    "# Hybrid with reranking\n",
    "print(\"\\n4Ô∏è‚É£ Hybrid + Reranking:\")\n",
    "reranked_results = reranker.rerank(hybrid_results, query=test_query)\n",
    "for i, result in enumerate(reranked_results[:3], 1):\n",
    "    print(f\"  {i}. {result.metadata['source']} - rerank score: {result.score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import RAGEvaluator, create_evaluator\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = create_evaluator()\n",
    "print(\"üìä RAGAS Evaluator initialized\\n\")\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What is hybrid retrieval?\",\n",
    "        \"expected_answer\": \"Hybrid retrieval combines dense vector embeddings and sparse keyword matching for optimal search results.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does reranking help?\",\n",
    "        \"expected_answer\": \"Reranking improves accuracy by re-scoring top results using a cross-encoder model.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üß™ Running {len(test_cases)} evaluation test cases...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    response = rag_chain.query(test[\"question\"], top_k=3)\n",
    "    \n",
    "    metrics = evaluator.evaluate_single(\n",
    "        question=test[\"question\"],\n",
    "        answer=response.answer,\n",
    "        expected=test[\"expected_answer\"],\n",
    "        contexts=[doc.text for doc in response.sources]\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        \"question\": test[\"question\"],\n",
    "        **metrics\n",
    "    })\n",
    "    \n",
    "    print(f\"Test {i}: {test['question'][:40]}...\")\n",
    "    print(f\"  Faithfulness: {metrics['faithfulness']:.2f}\")\n",
    "    print(f\"  Answer Relevancy: {metrics['answer_relevancy']:.2f}\")\n",
    "    print(f\"  Context Precision: {metrics['context_precision']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_queries = [\n",
    "    \"What is hybrid retrieval?\",\n",
    "    \"How does cross-encoder reranking work?\",\n",
    "    \"What are RAGAS metrics?\"\n",
    "]\n",
    "\n",
    "configs = [\n",
    "    (\"Dense Only\", {\"use_reranking\": False, \"use_sparse\": False}),\n",
    "    (\"Sparse Only\", {\"use_reranking\": False, \"use_dense\": False}),\n",
    "    (\"Hybrid (No Rerank)\", {\"use_reranking\": False}),\n",
    "    (\"Hybrid + Rerank\", {\"use_reranking\": True})\n",
    "]\n",
    "\n",
    "print(\"‚ö° Performance Comparison\\n\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for config_name, config in configs:\n",
    "    times = []\n",
    "    \n",
    "    for query in test_queries:\n",
    "        start = time.time()\n",
    "        _ = rag_chain.query(query, **config)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"{config_name:25} | Avg: {avg_time:.3f}s | Min: {min(times):.3f}s | Max: {max(times):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"Interactive chat with the RAG system.\"\"\"\n",
    "    print(\"üí¨ Enterprise-RAG Chat Interface\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nYou: \")\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye! üëã\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        response = rag_chain.query(query, top_k=3, use_reranking=True)\n",
    "        \n",
    "        print(f\"\\nAssistant: {response.answer}\")\n",
    "        \n",
    "        if response.sources:\n",
    "            print(\"\\nSources:\")\n",
    "            for source in response.sources:\n",
    "                print(f\"  - {source.metadata['source']}\")\n",
    "\n",
    "# Uncomment to run interactive chat\n",
    "# interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ What We Demonstrated:\n",
    "\n",
    "1. **System Setup** - Initialized all RAG components\n",
    "2. **Document Ingestion** - Processed multi-format documents\n",
    "3. **Hybrid Retrieval** - Combined dense + sparse search\n",
    "4. **Cross-Encoder Reranking** - Improved result accuracy\n",
    "5. **RAGAS Evaluation** - Measured faithfulness, relevancy, precision\n",
    "6. **Performance Analysis** - Compared different retrieval strategies\n",
    "\n",
    "### üéØ Key Features:\n",
    "\n",
    "- ‚úÖ Hybrid retrieval (dense + sparse)\n",
    "- ‚úÖ Cross-encoder reranking\n",
    "- ‚úÖ Multi-format document support\n",
    "- ‚úÖ RAGAS evaluation metrics\n",
    "- ‚úÖ Streaming responses\n",
    "- ‚úÖ Production-ready API\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "- Try with your own documents\n",
    "- Experiment with different embedding models\n",
    "- Tune reranking thresholds\n",
    "- Deploy with FastAPI: `uvicorn src.api.main:app --reload`\n",
    "- Explore the Streamlit UI: `streamlit run src/ui/app.py`\n",
    "\n",
    "---\n",
    "\n",
    "**üìñ Documentation:** [Enterprise-RAG README](../../projects/rag/Enterprise-RAG/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
